{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f11e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this file, I will add the features of the surrounding points into each point. \n",
    "#meaning, a point in the middle would have its own air temp, plus the 9 surrounding air temperatures\n",
    "#i will repeat that for each feature, except the x and y coordinates. \n",
    "#Hopefully, this will increase our ability to predict velocity. \n",
    "\n",
    "#I will leave the data as its original square with the fill values, and add the new columns. I will then select\n",
    "#for just the ice sheet/areas where there is velocity by removing rows where ice_velocity = NaN\n",
    "\n",
    "#IMPORTANT NOTE: I have created a slightly complicated naming system for neighbouring cells' features. \n",
    "#they are numbered 1-8 around the current cell, with 1 being the top left hand corner and counting clockwise\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('../data/AIS-86Year/vars_all_v2/vars-2015.txt', delimiter = '\\t',\n",
    "                      names=[\"x-axis\", \"y-axis\", \"ice_thickness\", \"ice_velocity\", \"ice_mask\", \n",
    "                             \"precipitation\", \"air_temp\", \"ocean_temp\"])\n",
    "\n",
    "df.head() \n",
    "\n",
    "precip_df = df[['x-axis', 'y-axis', 'precipitation']].copy()\n",
    "air_temp_df = df[['x-axis', 'y-axis', 'air_temp']].copy()\n",
    "ocean_temp_df = df[['x-axis', 'y-axis', 'ocean_temp']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83e4295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_neighbor_features(df, feature_col, prefix):\n",
    "    # Create a dictionary for faster lookup\n",
    "    lookup = {(x, y): df.loc[i, feature_col] for i, (x, y) in enumerate(zip(df['x-axis'], df['y-axis']))}\n",
    "    k = 121600  # Scale factor\n",
    "    relative_positions = [(-1*k, 1*k), (0*k, 1*k), (1*k, 1*k), (1*k, 0*k), (1*k, -1*k), \n",
    "                          (0*k, -1*k), (-1*k, -1*k), (-1*k, 0*k)]\n",
    "\n",
    "    # New DataFrame for results\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # Iterate over rows\n",
    "    for index, row in df.iterrows():\n",
    "        x, y = row['x-axis'], row['y-axis']\n",
    "        for i, (dx, dy) in enumerate(relative_positions, 1):\n",
    "            nx, ny = x + dx, y + dy\n",
    "            neighbor_feature = lookup.get((nx, ny), np.nan)\n",
    "            new_df.loc[index, f'{prefix}_{i}'] = neighbor_feature\n",
    "\n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75768c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x-axis</th>\n",
       "      <th>y-axis</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>precip_1</th>\n",
       "      <th>precip_2</th>\n",
       "      <th>precip_3</th>\n",
       "      <th>precip_4</th>\n",
       "      <th>precip_5</th>\n",
       "      <th>precip_6</th>\n",
       "      <th>precip_7</th>\n",
       "      <th>...</th>\n",
       "      <th>ocean_temp</th>\n",
       "      <th>ocean_temp_1</th>\n",
       "      <th>ocean_temp_2</th>\n",
       "      <th>ocean_temp_3</th>\n",
       "      <th>ocean_temp_4</th>\n",
       "      <th>ocean_temp_5</th>\n",
       "      <th>ocean_temp_6</th>\n",
       "      <th>ocean_temp_7</th>\n",
       "      <th>ocean_temp_8</th>\n",
       "      <th>ice_velocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>-243200</td>\n",
       "      <td>2067200</td>\n",
       "      <td>404.280640</td>\n",
       "      <td>396.511230</td>\n",
       "      <td>434.959656</td>\n",
       "      <td>436.197693</td>\n",
       "      <td>527.547485</td>\n",
       "      <td>461.252808</td>\n",
       "      <td>228.084351</td>\n",
       "      <td>186.801819</td>\n",
       "      <td>...</td>\n",
       "      <td>272.344940</td>\n",
       "      <td>272.423370</td>\n",
       "      <td>272.451752</td>\n",
       "      <td>272.460266</td>\n",
       "      <td>272.534760</td>\n",
       "      <td>272.646973</td>\n",
       "      <td>272.485016</td>\n",
       "      <td>272.184448</td>\n",
       "      <td>272.384827</td>\n",
       "      <td>4.782738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>-121600</td>\n",
       "      <td>2067200</td>\n",
       "      <td>527.547485</td>\n",
       "      <td>434.959656</td>\n",
       "      <td>436.197693</td>\n",
       "      <td>403.362183</td>\n",
       "      <td>352.272278</td>\n",
       "      <td>247.852112</td>\n",
       "      <td>461.252808</td>\n",
       "      <td>228.084351</td>\n",
       "      <td>...</td>\n",
       "      <td>272.534760</td>\n",
       "      <td>272.451752</td>\n",
       "      <td>272.460266</td>\n",
       "      <td>272.468903</td>\n",
       "      <td>272.597290</td>\n",
       "      <td>272.825623</td>\n",
       "      <td>272.646973</td>\n",
       "      <td>272.485016</td>\n",
       "      <td>272.344940</td>\n",
       "      <td>0.017201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0</td>\n",
       "      <td>2067200</td>\n",
       "      <td>352.272278</td>\n",
       "      <td>436.197693</td>\n",
       "      <td>403.362183</td>\n",
       "      <td>437.285950</td>\n",
       "      <td>261.181396</td>\n",
       "      <td>137.003937</td>\n",
       "      <td>247.852112</td>\n",
       "      <td>461.252808</td>\n",
       "      <td>...</td>\n",
       "      <td>272.597290</td>\n",
       "      <td>272.460266</td>\n",
       "      <td>272.468903</td>\n",
       "      <td>272.928741</td>\n",
       "      <td>273.296387</td>\n",
       "      <td>273.108215</td>\n",
       "      <td>272.825623</td>\n",
       "      <td>272.646973</td>\n",
       "      <td>272.534760</td>\n",
       "      <td>590.315247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>121600</td>\n",
       "      <td>2067200</td>\n",
       "      <td>261.181396</td>\n",
       "      <td>403.362183</td>\n",
       "      <td>437.285950</td>\n",
       "      <td>388.587189</td>\n",
       "      <td>358.309296</td>\n",
       "      <td>208.756500</td>\n",
       "      <td>137.003937</td>\n",
       "      <td>247.852112</td>\n",
       "      <td>...</td>\n",
       "      <td>273.296387</td>\n",
       "      <td>272.468903</td>\n",
       "      <td>272.928741</td>\n",
       "      <td>273.101837</td>\n",
       "      <td>273.666199</td>\n",
       "      <td>273.338440</td>\n",
       "      <td>273.108215</td>\n",
       "      <td>272.825623</td>\n",
       "      <td>272.597290</td>\n",
       "      <td>45.191998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>243200</td>\n",
       "      <td>2067200</td>\n",
       "      <td>358.309296</td>\n",
       "      <td>437.285950</td>\n",
       "      <td>388.587189</td>\n",
       "      <td>372.779785</td>\n",
       "      <td>101.287170</td>\n",
       "      <td>24.054939</td>\n",
       "      <td>208.756500</td>\n",
       "      <td>137.003937</td>\n",
       "      <td>...</td>\n",
       "      <td>273.666199</td>\n",
       "      <td>272.928741</td>\n",
       "      <td>273.101837</td>\n",
       "      <td>272.887238</td>\n",
       "      <td>273.610138</td>\n",
       "      <td>273.437042</td>\n",
       "      <td>273.338440</td>\n",
       "      <td>273.108215</td>\n",
       "      <td>273.296387</td>\n",
       "      <td>17.135721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x-axis   y-axis  precipitation    precip_1    precip_2    precip_3  \\\n",
       "431 -243200  2067200     404.280640  396.511230  434.959656  436.197693   \n",
       "432 -121600  2067200     527.547485  434.959656  436.197693  403.362183   \n",
       "433       0  2067200     352.272278  436.197693  403.362183  437.285950   \n",
       "434  121600  2067200     261.181396  403.362183  437.285950  388.587189   \n",
       "435  243200  2067200     358.309296  437.285950  388.587189  372.779785   \n",
       "\n",
       "       precip_4    precip_5    precip_6    precip_7  ...  ocean_temp  \\\n",
       "431  527.547485  461.252808  228.084351  186.801819  ...  272.344940   \n",
       "432  352.272278  247.852112  461.252808  228.084351  ...  272.534760   \n",
       "433  261.181396  137.003937  247.852112  461.252808  ...  272.597290   \n",
       "434  358.309296  208.756500  137.003937  247.852112  ...  273.296387   \n",
       "435  101.287170   24.054939  208.756500  137.003937  ...  273.666199   \n",
       "\n",
       "     ocean_temp_1  ocean_temp_2  ocean_temp_3  ocean_temp_4  ocean_temp_5  \\\n",
       "431    272.423370    272.451752    272.460266    272.534760    272.646973   \n",
       "432    272.451752    272.460266    272.468903    272.597290    272.825623   \n",
       "433    272.460266    272.468903    272.928741    273.296387    273.108215   \n",
       "434    272.468903    272.928741    273.101837    273.666199    273.338440   \n",
       "435    272.928741    273.101837    272.887238    273.610138    273.437042   \n",
       "\n",
       "     ocean_temp_6  ocean_temp_7  ocean_temp_8  ice_velocity  \n",
       "431    272.485016    272.184448    272.384827      4.782738  \n",
       "432    272.646973    272.485016    272.344940      0.017201  \n",
       "433    272.825623    272.646973    272.534760    590.315247  \n",
       "434    273.108215    272.825623    272.597290     45.191998  \n",
       "435    273.338440    273.108215    273.296387     17.135721  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting neighbour columns \n",
    "precip_neighbours = add_neighbor_features(precip_df, 'precipitation', 'precip')\n",
    "air_temp_neighbours = add_neighbor_features(air_temp_df, 'air_temp', 'air_temp')\n",
    "ocean_temp_neighbours = add_neighbor_features(ocean_temp_df, 'ocean_temp', 'ocean_temp')\n",
    "\n",
    "#joining all the tables into one mega table \n",
    "precip_neighbours = precip_neighbours.drop(columns = ['x-axis','y-axis'])\n",
    "air_temp_neighbours = air_temp_neighbours.drop(columns = ['x-axis','y-axis'])\n",
    "ocean_temp_neighbours = ocean_temp_neighbours.drop(columns = ['x-axis','y-axis'])\n",
    "\n",
    "concat_df = df[['x-axis', 'y-axis', 'ice_velocity']].copy()\n",
    "\n",
    "result_df = pd.concat([concat_df, precip_neighbours, air_temp_neighbours, ocean_temp_neighbours], axis=1)\n",
    "result_df.head()\n",
    "\n",
    "#reshuffle final df so target is last column\n",
    "\n",
    "result_df = result_df[['x-axis','y-axis','precipitation', 'precip_1', 'precip_2', 'precip_3', 'precip_4', \n",
    "                      'precip_5', 'precip_6', 'precip_7', 'precip_8', 'air_temp', 'air_temp_1', 'air_temp_2', \n",
    "                      'air_temp_3', 'air_temp_4', 'air_temp_5', 'air_temp_6', 'air_temp_7', 'air_temp_8', \n",
    "                      'ocean_temp', 'ocean_temp_1', 'ocean_temp_2', 'ocean_temp_3', 'ocean_temp_4', \n",
    "                      'ocean_temp_5', 'ocean_temp_6', 'ocean_temp_7', 'ocean_temp_8', 'ice_velocity']]\n",
    "\n",
    "\n",
    "result_df.dropna(subset=[\"ice_velocity\"], inplace=True)\n",
    " \n",
    "result_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e580ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++ RF REGRESSION +++++++++++++++++++++++++++++++++++++++++++\n",
    "def doRandomForest(preprocessed_df, seed, n_runs, target_name, original_df):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    train_RMSEs = []\n",
    "    test_RMSEs = []\n",
    "    importances = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        randomState = i * seed\n",
    "        X = preprocessed_df.iloc[:, :-1]\n",
    "        y = preprocessed_df.iloc[:, -1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=randomState)\n",
    "\n",
    "        model = RandomForestRegressor(random_state=randomState)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        #, max_features = 2\n",
    "        # Store metrics for training set\n",
    "        train_scores.append(model.score(X_train, y_train))\n",
    "        y_train_predicted = model.predict(X_train)\n",
    "        train_scaled_rmse = math.sqrt(mean_squared_error(y_train, y_train_predicted))\n",
    "        train_rmse = unscale_rmse(train_scaled_rmse, target_name, original_df)\n",
    "        train_RMSEs.append(train_rmse)\n",
    "\n",
    "        # Store metrics for test set\n",
    "        test_scores.append(model.score(X_test, y_test))\n",
    "        importances.append(model.feature_importances_)\n",
    "        y_test_predicted = model.predict(X_test)\n",
    "        test_scaled_rmse = math.sqrt(mean_squared_error(y_test, y_test_predicted))\n",
    "        test_rmse = unscale_rmse(test_scaled_rmse, target_name, original_df)\n",
    "        test_RMSEs.append(test_rmse)\n",
    "\n",
    "    # Calculate the average and standard deviation for each metric\n",
    "    avg_train_score, std_train_score = np.mean(train_scores), np.std(train_scores)\n",
    "    avg_test_score, std_test_score = np.mean(test_scores), np.std(test_scores)\n",
    "    avg_train_rmse, std_train_rmse = np.mean(train_RMSEs), np.std(train_RMSEs)\n",
    "    avg_test_rmse, std_test_rmse = np.mean(test_RMSEs), np.std(test_RMSEs)\n",
    "    avg_importances, std_importances = np.mean(importances, axis=0), np.std(importances, axis=0)\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Training Average Score: {avg_train_score:.4f}, Std Dev: {std_train_score:.4f}\")\n",
    "    print(f\"Training Average RMSE: {avg_train_rmse:.2f}, Std Dev: {std_train_rmse:.2f}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Test Average Score: {avg_test_score:.4f}, Std Dev: {std_test_score:.4f}\")\n",
    "    print(f\"Test Average RMSE: {avg_test_rmse:.2f}, Std Dev: {std_test_rmse:.2f}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Average Importances:\", avg_importances)\n",
    "    print(\"Importances Std Dev:\", std_importances)\n",
    "    print(f\"\\n\\n Train-Test (Difference): {avg_train_score - avg_test_score :.4f}\")\n",
    "\n",
    "\n",
    "    return avg_importances, std_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7af76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale (df):\n",
    "    \"\"\"uses the formula scaled val = (val-column minimum)/(column maximum - column minimum)\"\"\"\n",
    "    scaled_df = df.copy()\n",
    "    # for column in df.columns[:-1]  -> use this line instead for not having a scaled target (for ice mask)\n",
    "    for column in scaled_df.columns:\n",
    "        min_value = scaled_df[column].min()\n",
    "        max_value = scaled_df[column].max()\n",
    "        scaled_df[column] = (scaled_df[column] - min_value) / (max_value - min_value)\n",
    "            \n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "def unscale_rmse(scaled_rmse, target_name, original_df):\n",
    "    target_min = original_df[target_name].min()\n",
    "    target_max = original_df[target_name].max()\n",
    "\n",
    "    return scaled_rmse * (target_max - target_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cba4f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Average Score: 0.8731, Std Dev: 0.0162\n",
      "Training Average RMSE: 108.16, Std Dev: 9.29\n",
      "\n",
      "\n",
      "Test Average Score: 0.0697, Std Dev: 0.2069\n",
      "Test Average RMSE: 285.30, Std Dev: 41.96\n",
      "\n",
      "\n",
      "Average Importances: [0.01113265 0.02172083 0.04933474 0.03092435 0.03191351 0.01365272\n",
      " 0.10478029 0.02490995 0.03042098 0.02284384 0.02162971 0.112315\n",
      " 0.01282879 0.02040791 0.0180971  0.07074429 0.08846769 0.05324319\n",
      " 0.01478224 0.01501873 0.01999319 0.04314465 0.01312849 0.01275241\n",
      " 0.02658204 0.03186029 0.04403082 0.02126242 0.01807719]\n",
      "Importances Std Dev: [0.0035826  0.01359957 0.02717266 0.01300638 0.01671924 0.00478298\n",
      " 0.05394346 0.01298115 0.01345028 0.00773119 0.00996795 0.03799013\n",
      " 0.00405901 0.00992342 0.00669225 0.03538001 0.0397932  0.0371976\n",
      " 0.0041739  0.0060598  0.00711656 0.02300258 0.0033098  0.00369623\n",
      " 0.01315727 0.01947049 0.0196674  0.00796887 0.00924772]\n",
      "\n",
      "\n",
      " Train-Test (Difference): 0.8034\n"
     ]
    }
   ],
   "source": [
    "seed = 101\n",
    "result_df_scaled = scale(result_df)\n",
    "IV_importances, IV_std_importances = doRandomForest(result_df_scaled, seed, 30, 'ice_velocity', result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de03fd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10800 candidates, totalling 32400 fits\n",
      "Best parameters found:  {'max_depth': 30, 'max_features': 6, 'max_leaf_nodes': 200, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Test Score:  0.18830566830227813\n"
     ]
    }
   ],
   "source": [
    "#doing a grid search of best hyperparameters - note, not doing it in the same way as I have before where I do the\n",
    "#grid search 30 times and find the best params, as the RF with this many columns takes significantly longer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "result_df_scaled = scale(result_df)\n",
    "\n",
    "\n",
    "X = result_df_scaled.iloc[:, :-1]\n",
    "y = result_df_scaled.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "    \n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_features': [3, 6, 9, 12, 15, 18, 21, 24, 27],  # Number of features to consider at every split\n",
    "    'max_depth': [10, 20, 30, None],   # Maximum number of levels in tree\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],   # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 10],     # Minimum number of samples required at each leaf node\n",
    "    'max_leaf_nodes':[60, 90, 120, 200]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor(random_state = seed)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Test the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "test_score = best_rf.score(X_test, y_test)\n",
    "print(\"Test Score: \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25852259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after this, try a bunch of different models on the XL dataset - see if RF is still the most promising\n",
    "#also try adding in the features that I engineered on top of the XL dataset to see if that can make any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "output:\n",
    "Fitting 3 folds for each of 10800 candidates, totalling 32400 fits\n",
    "Best parameters found:  {'max_depth': 30, 'max_features': 6, 'max_leaf_nodes': 200, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "Test Score:  0.18830566830227813"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
