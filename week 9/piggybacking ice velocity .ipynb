{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df511e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dont know if this is something typically done, I assume so though\n",
    "# in this file I will experiment with how I can use ice mask to predict ice velocity\n",
    "# since the ice mask prediction is quite good, I thought I could predict ice mask, then add that predicted\n",
    "# column back in as a feature to be used in the ice velocity prediction. Since velocity is not being used\n",
    "# as a predictor of ice mask, i believe there should be no data leak from this. \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EF = pd.read_csv('../data/AIS_data/EF_full_df.csv')\n",
    "\n",
    "velocity = EF[['ice_velocity']]\n",
    "thickness = EF[['ice_thickness']]\n",
    "\n",
    "#shifting the columns so the target is the one furthest to the right -> needed for scaling to avoid scaling ice mask\n",
    "EF = EF[['x-axis', 'y-axis', 'precipitation', 'precip_roll', 'air_temp', 'air_roll', 'ocean_temp',\n",
    "         'ocean_roll', 'temp_diff', 'dist', 'ice_velocity', 'ice_thickness','ice_mask']]\n",
    "\n",
    "#method to normalise the data -> scale each column between 0 and 1 \n",
    "def scale (df):\n",
    "    \"\"\"uses the formula scaled val = (val-column minimum)/(column maximum - column minimum)\"\"\"\n",
    "    scaled_df = df.copy()\n",
    "    # for column in df.columns[:-1]  -> use this line instead for not having a scaled target \n",
    "    for column in scaled_df.columns[:-1]:\n",
    "        min_value = scaled_df[column].min()\n",
    "        max_value = scaled_df[column].max()\n",
    "        scaled_df[column] = (scaled_df[column] - min_value) / (max_value - min_value)\n",
    "            \n",
    "    return scaled_df\n",
    "\n",
    "def unscale_rmse(scaled_rmse, target_name, original_df):\n",
    "    target_min = original_df[target_name].min()\n",
    "    target_max = original_df[target_name].max()\n",
    "\n",
    "    return scaled_rmse * (target_max - target_min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32acd0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def doPolySVC(preprocessed_df, seed, n_runs):\n",
    "    overall_accuracies = []\n",
    "    training_accuracies = []\n",
    "    category_accuracies = {'grounded_ice': [], 'floating_ice': [], 'open_ocean': []}\n",
    "    training_category_accuracies = {'grounded_ice': [], 'floating_ice': [], 'open_ocean': []}\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        randomState = i * seed\n",
    "        X = preprocessed_df.iloc[:, :-1]\n",
    "        y = preprocessed_df.iloc[:, -1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=randomState)\n",
    "        \n",
    "        #then remove the ice velocity from both xy train and test, but keep aside \n",
    "        \n",
    "        model = SVC(kernel='poly', random_state=randomState, class_weight = 'balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Test data evaluation\n",
    "        y_predicted = model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_predicted)\n",
    "        overall_accuracies.append(test_accuracy)\n",
    "        \n",
    "        test_cm = confusion_matrix(y_test, y_predicted, labels=[2.0, 3.0, 4.0])\n",
    "        category_accuracies['grounded_ice'].append(test_cm[0, 0] / test_cm[0, :].sum() if test_cm[0, :].sum() > 0 else 0)\n",
    "        category_accuracies['floating_ice'].append(test_cm[1, 1] / test_cm[1, :].sum() if test_cm[1, :].sum() > 0 else 0)\n",
    "        category_accuracies['open_ocean'].append(test_cm[2, 2] / test_cm[2, :].sum() if test_cm[2, :].sum() > 0 else 0)\n",
    "\n",
    "        # Training data evaluation\n",
    "        y_train_predicted = model.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_predicted)\n",
    "        training_accuracies.append(train_accuracy)\n",
    "\n",
    "        train_cm = confusion_matrix(y_train, y_train_predicted, labels=[2.0, 3.0, 4.0])\n",
    "        training_category_accuracies['grounded_ice'].append(train_cm[0, 0] / train_cm[0, :].sum() if train_cm[0, :].sum() > 0 else 0)\n",
    "        training_category_accuracies['floating_ice'].append(train_cm[1, 1] / train_cm[1, :].sum() if train_cm[1, :].sum() > 0 else 0)\n",
    "        training_category_accuracies['open_ocean'].append(train_cm[2, 2] / train_cm[2, :].sum() if train_cm[2, :].sum() > 0 else 0)\n",
    "\n",
    "    # Calculate averages and standard deviations\n",
    "    average_accuracy = np.mean(overall_accuracies)\n",
    "    std_accuracy = np.std(overall_accuracies)\n",
    "    average_training_accuracy = np.mean(training_accuracies)\n",
    "    std_training_accuracy = np.std(training_accuracies)\n",
    "\n",
    "    average_category_accuracies = {k: np.mean(v) for k, v in category_accuracies.items()}\n",
    "    std_category_accuracies = {k: np.std(v) for k, v in category_accuracies.items()}\n",
    "    average_training_category_accuracies = {k: np.mean(v) for k, v in training_category_accuracies.items()}\n",
    "    std_training_category_accuracies = {k: np.std(v) for k, v in training_category_accuracies.items()}\n",
    "\n",
    "    # Print average and standard deviation of accuracies\n",
    "    print(f\"Average Training Accuracy: {average_training_accuracy:.3f}, Std: {std_training_accuracy:.3f}\")\n",
    "    print(\"Training Category Metrics:\")\n",
    "    for category in training_category_accuracies:\n",
    "        print(f\"  {category}: Avg: {average_training_category_accuracies[category]:.3f}, Std: {std_training_category_accuracies[category]:.3f}\") \n",
    "\n",
    "    print(f\"\\nAverage Test Accuracy: {average_accuracy:.3f}, Std: {std_accuracy:.3f}\")\n",
    "    print(\"Test Category Metrics:\")\n",
    "    for category in category_accuracies:\n",
    "        print(f\"  {category}: Avg: {average_category_accuracies[category]:.3f}, Std: {std_category_accuracies[category]:.3f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd47aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_velocity\n",
      "1623    0.000000\n",
      "1103    0.000000\n",
      "1851    0.004555\n",
      "1933    0.001471\n",
      "113     0.000000\n",
      "          ...   \n",
      "1033    0.000512\n",
      "1731    0.000000\n",
      "763     0.000685\n",
      "835     0.001269\n",
      "1653    0.010505\n",
      "Name: ice_velocity, Length: 1579, dtype: float64\n",
      "X train\n",
      "      x-axis  y-axis  precipitation  precip_roll  air_temp  air_roll  \\\n",
      "1623    0.20    0.30       0.303468     0.486340  0.861506  0.861147   \n",
      "1103    0.00    0.50       0.338658     0.540796  0.978627  0.977795   \n",
      "1851    0.80    0.22       0.260878     0.500682  0.543994  0.588928   \n",
      "1933    0.62    0.18       0.107907     0.207050  0.428930  0.475921   \n",
      "113     0.18    0.92       0.283395     0.433198  0.900400  0.897791   \n",
      "...      ...     ...            ...          ...       ...       ...   \n",
      "1033    0.64    0.54       0.016987     0.001387  0.007904  0.000730   \n",
      "1731    0.34    0.26       0.294340     0.479494  0.819511  0.818413   \n",
      "763     0.34    0.64       0.077438     0.132742  0.531724  0.555130   \n",
      "835     0.76    0.62       0.036935     0.041597  0.432707  0.395471   \n",
      "1653    0.80    0.30       0.082163     0.126328  0.363096  0.356787   \n",
      "\n",
      "      ocean_temp  ocean_roll  temp_diff      dist  \n",
      "1623    0.321070    0.327467   0.099363  0.660043  \n",
      "1103    0.574947    0.603218   0.001379  0.915315  \n",
      "1851    0.096582    0.178509   0.413125  0.751228  \n",
      "1933    0.062901    0.066764   0.531776  0.625636  \n",
      "113     0.303703    0.335831   0.056273  0.966600  \n",
      "...          ...         ...        ...       ...  \n",
      "1033    0.168466    0.165597   0.989675  0.266544  \n",
      "1731    0.209340    0.210621   0.132349  0.528034  \n",
      "763     0.007981    0.019781   0.416956  0.389197  \n",
      "835     0.258134    0.228041   0.548027  0.524213  \n",
      "1653    0.235868    0.200033   0.619613  0.660043  \n",
      "\n",
      "[1579 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# NOTE NEED TO DO STRATIFIED SPLITTING SINCE THE CLASS IS SO IMBALANCED \n",
    "def doPiggyBack(preprocessed_df, seed, n_runs):\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        randomState = i * seed\n",
    "        X = preprocessed_df.iloc[:, :-1]\n",
    "        y = preprocessed_df.iloc[:, -1]\n",
    "        \n",
    "        #split into training and test \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=randomState)\n",
    "        \n",
    "        #then remove the ice velocity from both xy train and test, but keep aside \n",
    "        X_train_velocity = X_train['ice_velocity']\n",
    "        X_train.drop(columns = ['ice_velocity'], inplace = True)\n",
    "        X_test_velocity = X_test['ice_velocity']\n",
    "        X_test.drop(columns = ['ice_velocity'], inplace = True)\n",
    "        \n",
    "        model = SVC(kernel='poly', random_state=randomState, class_weight = 'balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Training data evaluation\n",
    "        y_train_predicted = model.predict(X_train)\n",
    "        \n",
    "        # Test data evaluation\n",
    "        y_test_predicted = model.predict(X_test)\n",
    "        \n",
    "        #adding the data back together\n",
    "        \n",
    "        \n",
    "seed = 101\n",
    "preprocessed_df = EF.drop(columns = ['ice_thickness'], inplace = False)\n",
    "preprocessed_df.fillna(value = -1, inplace = True)\n",
    "scaled_preprocessed_df = scale(preprocessed_df)\n",
    "scaled_preprocessed_df.head()  \n",
    "\n",
    "doPiggyBack(scaled_preprocessed_df, seed, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e586097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
